{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "DataReader module for reading data from various sources into pandas DataFrames.\n",
    "\n",
    "Dependencies:\n",
    "- pandas: Required for DataFrame operations.\n",
    "- requests: Required for API reading.\n",
    "- sqlalchemy: Required for database reading.\n",
    "- pyarrow: Optional for .parquet file support.\n",
    "- openpyxl: Optional for .xlsx file support.\n",
    "\n",
    "Usage:\n",
    "    reader = DataReader()\n",
    "    source = DataSource(url=\"data.csv\")\n",
    "    df = reader.read_data(source)\n",
    "\n",
    "DataReader         ->   it is used to initialise registry\n",
    "DataSource         ->   validate the url and store the parsed result\n",
    "Reader             ->   Read the data and return supported formats\n",
    "ReaderRegistry     ->   Registers the file type we will be using\n",
    "Configure Logging  ->   Configure the DataReader logger with console and optional file handlers.\n",
    "\"\"\"\n",
    "__all__ = ['DataReader', 'DataSource', 'Reader', 'ReaderRegistry', 'configure_logging']\n",
    "\n",
    "import logging\n",
    "import os\n",
    "import json\n",
    "from abc import ABC, abstractmethod\n",
    "from urllib.parse import urlparse, ParseResult\n",
    "import pandas as pd\n",
    "import requests\n",
    "from sqlalchemy import create_engine\n",
    "from typing import Callable, Optional,Set\n",
    "from dataclasses import dataclass\n",
    "from io import StringIO, BytesIO\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def configure_logging(\n",
    "    level: int = logging.WARNING,\n",
    "    log_file: Optional[str] = 'datareader.log',\n",
    "    logging_enabled: bool = True,\n",
    "    debug_enabled: bool = False\n",
    ") -> logging.Logger:\n",
    "    \"\"\"\n",
    "    Configure the DataReader logger with console and optional file handlers.\n",
    "\n",
    "    Args:\n",
    "        level: Logging level (e.g., logging.WARNING, logging.INFO). Defaults to WARNING.\n",
    "        log_file: File path for file logging. Defaults to 'datareader.log'. If None, logs to console only.\n",
    "        logging_enabled: If False, disables all logging by setting level to CRITICAL+1. Defaults to True.\n",
    "        debug_enabled: If True, enables DEBUG logging for detailed output. Defaults to False.\n",
    "                       Must be explicitly set to True to enable DEBUG logs.\n",
    "\n",
    "    Returns:\n",
    "        Configured logger instance.\n",
    "    \"\"\"\n",
    "    logger = logging.getLogger('DataReader')\n",
    "    logger.handlers.clear()\n",
    "\n",
    "    if not logging_enabled:\n",
    "        logger.setLevel(logging.CRITICAL + 1)\n",
    "        return logger\n",
    "\n",
    "    effective_level = logging.DEBUG if debug_enabled else level\n",
    "    logger.setLevel(effective_level)\n",
    "\n",
    "    console_handler = logging.StreamHandler()\n",
    "    console_handler.setLevel(effective_level)\n",
    "    console_handler.setFormatter(logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s'))\n",
    "    logger.addHandler(console_handler)\n",
    "\n",
    "    if log_file:\n",
    "        file_handler = logging.FileHandler(log_file)\n",
    "        file_handler.setLevel(effective_level)\n",
    "        file_handler.setFormatter(logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s'))\n",
    "        logger.addHandler(file_handler)\n",
    "\n",
    "    return logger\n",
    "\n",
    "logger = configure_logging()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Reader(ABC):\n",
    "    @abstractmethod\n",
    "    def read(self, url: str, **kwargs) -> pd.DataFrame:\n",
    "        \"\"\"Read data from a source into a DataFrame.\"\"\"\n",
    "        pass\n",
    "\n",
    "    @abstractmethod\n",
    "    def get_supported_formats(self) -> set[str]:\n",
    "        \"\"\"Return supported formats or schemes.\"\"\"\n",
    "        pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class ApiReader(Reader):\n",
    "    def read(self, url: str, **kwargs) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Read data from an HTTP/HTTPS API endpoint into a DataFrame.\n",
    "\n",
    "        Args:\n",
    "            url: The API endpoint.\n",
    "            **kwargs: Optional parameters such as headers, params, or response_format.\n",
    "\n",
    "        Returns:\n",
    "            pandas DataFrame containing the API response.\n",
    "        \"\"\"\n",
    "        logger.debug(\"Requesting API URL: %s with kwargs: %s\", url, kwargs)\n",
    "        response = requests.get(url, **kwargs)\n",
    "        response.raise_for_status()\n",
    "\n",
    "        content_type = response.headers.get(\"Content-Type\", \"\")\n",
    "        if \"application/json\" in content_type:\n",
    "            data = response.json()\n",
    "            return pd.DataFrame(data)\n",
    "        elif \"text/csv\" in content_type:\n",
    "            from io import StringIO\n",
    "            return pd.read_csv(StringIO(response.text))\n",
    "        else:\n",
    "            raise ValueError(f\"Unsupported Content-Type: {content_type}\")\n",
    "\n",
    "    def validate(self, url: str) -> bool:\n",
    "        \"\"\"\n",
    "        Validate the URL format to ensure it's HTTP or HTTPS.\n",
    "        \"\"\"\n",
    "        return url.startswith((\"http://\", \"https://\"))\n",
    "\n",
    "    def get_supported_formats(self) -> Set[str]:\n",
    "        \"\"\"\n",
    "        Return supported URL schemes.\n",
    "        \"\"\"\n",
    "        return {\"http\", \"https\"}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "#used to SQL databases \n",
    "class DatabaseReader(Reader):\n",
    "    def read(self, url: str, **kwargs) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Read data from a database into a DataFrame.\n",
    "\n",
    "        Args:\n",
    "            url: Database connection string (e.g., 'sqlite:///db.sqlite').\n",
    "            **kwargs: Optional parameters (e.g., query as a non-empty string, default: 'SELECT * FROM data_table').\n",
    "\n",
    "        Returns:\n",
    "            pandas DataFrame with query results.\n",
    "\n",
    "        Raises:\n",
    "            ValueError: If the URL or query is invalid.\n",
    "            sqlalchemy.exc.SQLAlchemyError: If the query fails.\n",
    "        \"\"\"\n",
    "        query = kwargs.get(\"query\", \"SELECT * FROM data_table\")\n",
    "        logger.debug(\"Reading database: %s with query: %s\", url, query)\n",
    "        if \"query\" in kwargs and (not isinstance(query, str) or not query.strip()):\n",
    "            logger.error(\"Invalid query: %s\", query)\n",
    "            raise ValueError(f\"Invalid query: {query}\")\n",
    "        if not self.validate(url):\n",
    "            logger.error(\"Invalid database URL: %s\", url)\n",
    "            raise ValueError(f\"Invalid database URL: {url}\")\n",
    "        engine = create_engine(url)\n",
    "        df = pd.read_sql(query, engine)\n",
    "        logger.info(\"Successfully read database data from %s\", url)\n",
    "        return df\n",
    "\n",
    "    def validate(self, url: str) -> bool:\n",
    "        \"\"\"Check if the URL is a valid database connection string.\"\"\"\n",
    "        return url.startswith((\"sqlite://\", \"postgresql://\", \"mysql://\"))\n",
    "\n",
    "    def get_supported_formats(self) -> set[str]:\n",
    "        \"\"\"Return supported schemes.\"\"\"\n",
    "        return {\"sqlite\", \"postgresql\", \"mysql\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "#it is used to read from files (csv,xlsx,json,parquet)\n",
    "class FileReader(Reader):\n",
    "    def read(self, path: str, **kwargs) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Read data from a file into a DataFrame.\n",
    "\n",
    "        Args:\n",
    "            path: File path (e.g., 'data.csv').\n",
    "            **kwargs: Optional parameters for file reading (e.g., encoding as a string).\n",
    "\n",
    "        Returns:\n",
    "            pandas DataFrame with file data.\n",
    "\n",
    "        Raises:\n",
    "            FileNotFoundError: If the file does not exist.\n",
    "            ValueError: If the file format or encoding is invalid.\n",
    "        \"\"\"\n",
    "        logger.debug(\"Reading file: %s with kwargs: %s\", path, kwargs)\n",
    "        if \"encoding\" in kwargs and not isinstance(kwargs[\"encoding\"], str):\n",
    "            logger.error(\"Invalid encoding: %s\", kwargs[\"encoding\"])\n",
    "            raise ValueError(f\"Invalid encoding: {kwargs['encoding']}\")\n",
    "        if not os.path.exists(path):\n",
    "            logger.error(\"File not found: %s\", path)\n",
    "            raise FileNotFoundError(f\"File not found: {path}\")\n",
    "        file_readers = {\n",
    "            \".csv\": pd.read_csv,\n",
    "            \".json\": pd.read_json,\n",
    "            \".parquet\": pd.read_parquet,\n",
    "            \".xlsx\": pd.read_excel\n",
    "        }\n",
    "        for ext, reader in file_readers.items():\n",
    "            if path.endswith(ext):\n",
    "                logger.info(\"Reading %s with %s\", path, reader.__name__)\n",
    "                return reader(path, **kwargs)\n",
    "        logger.error(\"Unsupported file format: %s. Supported formats: %s\", path, list(file_readers.keys()))\n",
    "        raise ValueError(f\"Unsupported file format: {path}. Supported formats: {list(file_readers.keys())}\")\n",
    "    def get_supported_formats(self) -> set[str]:\n",
    "        return {\".csv\", \".json\", \".parquet\", \".xlsx\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CompressedFileReader(Reader):\n",
    "    \"\"\"\n",
    "    CompressedFileReader is used to read tabular data from compressed files into a pandas DataFrame.\n",
    "\n",
    "    Supported compression formats:\n",
    "        - .gz  → gzip\n",
    "        - .zip → zip\n",
    "        - .bz2 → bz2\n",
    "        - .xz  → xz\n",
    "\n",
    "    Methods:\n",
    "        - read(path: str, **kwargs): Reads and returns the contents of the compressed file as a DataFrame.\n",
    "        - validate(path: str): Validates if the file extension is supported.\n",
    "        - get_supported_formats(): Returns a set of supported file extensions.\n",
    "\n",
    "    Notes:\n",
    "        - Compression format is inferred **only** from the file extension.\n",
    "          Files with incorrect or missing extensions may result in errors or incorrect behavior.\n",
    "        \n",
    "        - File contents must be compatible with `pandas.read_csv()`.\n",
    "          If the file contains binary or non-tabular data, an exception will be raised.\n",
    "        \n",
    "        - Additional parameters (e.g., `sep`, `encoding`, `dtype`, etc.) can be passed via **kwargs to customize reading.\n",
    "    \"\"\"\n",
    "\n",
    "    def read(self, path: str, **kwargs) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Reads a compressed CSV file and returns it as a pandas DataFrame.\n",
    "\n",
    "        Parameters:\n",
    "            path (str): Path to the compressed file.\n",
    "            **kwargs: Additional keyword arguments for pandas.read_csv.\n",
    "\n",
    "        Returns:\n",
    "            pd.DataFrame: Loaded data.\n",
    "\n",
    "        Raises:\n",
    "            ValueError: If the compression format is unsupported.\n",
    "            Exception: If reading the file fails.\n",
    "        \"\"\"\n",
    "        logger.debug(\"Reading compressed file: %s\", path)\n",
    "\n",
    "        compression_type = self._infer_compression(path)\n",
    "        if compression_type is None:\n",
    "            logger.error(\"Unsupported compression format for file: %s\", path)\n",
    "            raise ValueError(f\"Unsupported compression format: {path}\")\n",
    "\n",
    "        try:\n",
    "            return pd.read_csv(path, compression=compression_type, **kwargs)\n",
    "        except Exception as e:\n",
    "            logger.error(\"Error reading compressed file %s: %s\", path, str(e))\n",
    "            raise\n",
    "\n",
    "    def validate(self, path: str) -> bool:\n",
    "        \"\"\"\n",
    "        Validates if the given file has a supported compression format.\n",
    "\n",
    "        Parameters:\n",
    "            path (str): Path to the file.\n",
    "\n",
    "        Returns:\n",
    "            bool: True if supported, False otherwise.\n",
    "        \"\"\"\n",
    "        return self._infer_compression(path) is not None\n",
    "\n",
    "    def get_supported_formats(self) -> set[str]:\n",
    "        \"\"\"\n",
    "        Returns a set of supported compressed file extensions.\n",
    "\n",
    "        Returns:\n",
    "            set[str]: Supported extensions.\n",
    "        \"\"\"\n",
    "        return {\".gz\", \".zip\", \".bz2\", \".xz\"}\n",
    "\n",
    "    def _infer_compression(self, path: str) -> str | None:\n",
    "        \"\"\"\n",
    "        Infers the compression format based on file extension.\n",
    "\n",
    "        Parameters:\n",
    "            path (str): File path.\n",
    "\n",
    "        Returns:\n",
    "            str | None: Compression format string or None if unsupported.\n",
    "        \"\"\"\n",
    "        ext = os.path.splitext(path)[1].lower()\n",
    "        mapping = {\n",
    "            \".gz\": \"gzip\",\n",
    "            \".zip\": \"zip\",\n",
    "            \".bz2\": \"bz2\",\n",
    "            \".xz\": \"xz\"\n",
    "        }\n",
    "        return mapping.get(ext)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "class URLReader(Reader):\n",
    "    def read(self, url: str, **kwargs) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Read data from a URL into a pandas DataFrame.\n",
    "\n",
    "        Args:\n",
    "            url: The URL pointing to the data file.\n",
    "            **kwargs: Optional parameters like 'file_type' (csv, json, excel).\n",
    "\n",
    "        Returns:\n",
    "            pandas DataFrame with data loaded from the URL.\n",
    "\n",
    "        Raises:\n",
    "            ValueError: If the URL or file type is invalid.\n",
    "            requests.exceptions.RequestException: If the download fails.\n",
    "            pd.errors.ParserError: If pandas fails to parse the data.\n",
    "        \"\"\"\n",
    "        file_type = kwargs.pop(\"file_type\", None)\n",
    "        logger.debug(\"Downloading data from URL: %s\", url)\n",
    "        \n",
    "        response = requests.get(url)\n",
    "        response.raise_for_status()\n",
    "\n",
    "        if not file_type:\n",
    "            if url.endswith(\".csv\"):\n",
    "                file_type = \"csv\"\n",
    "            elif url.endswith(\".json\"):\n",
    "                file_type = \"json\"\n",
    "            elif url.endswith(\".xlsx\") or url.endswith(\".xls\"):\n",
    "                file_type = \"excel\"\n",
    "            else:\n",
    "                logger.error(\"Could not infer file type from URL. Please specify 'file_type'\")\n",
    "                raise ValueError(\"Unsupported or unknown file type. Please specify 'file_type' (csv, json, excel).\")\n",
    "\n",
    "        logger.debug(\"Reading data as %s\", file_type)\n",
    "\n",
    "        if file_type == \"csv\":\n",
    "            data = pd.read_csv(StringIO(response.text), **kwargs)\n",
    "        elif file_type == \"json\":\n",
    "            data = pd.read_json(StringIO(response.text), **kwargs)\n",
    "        elif file_type == \"excel\":\n",
    "            data = pd.read_excel(BytesIO(response.content), **kwargs)\n",
    "        else:\n",
    "            logger.error(\"Unsupported file type requested: %s\", file_type)\n",
    "            raise ValueError(f\"Unsupported file type: {file_type}\")\n",
    "\n",
    "        logger.info(\"Successfully read data from URL: %s\", url)\n",
    "        return data\n",
    "\n",
    "    def validate(self, url: str) -> bool:\n",
    "        \"\"\"Check if the URL is a valid HTTP/HTTPS URL.\"\"\"\n",
    "        return url.startswith((\"http://\", \"https://\"))\n",
    "\n",
    "    def get_supported_formats(self) -> set[str]:\n",
    "        \"\"\"Return supported file types for URLReader.\"\"\"\n",
    "        return {\"csv\", \"json\", \"excel\"}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "#it is used to check if the url or file path exists and if its valid  \n",
    "@dataclass #decorator which is in built in python with various functions \n",
    "class DataSource:\n",
    "    url: str\n",
    "    _parsed: Optional[ParseResult] = None\n",
    "\n",
    "    def __post_init__(self):\n",
    "        \"\"\"Validate the URL and store parsed result.\"\"\"\n",
    "        logger.debug(\"Validating DataSource URL: %s\", self.url)\n",
    "        if not self.url or not isinstance(self.url, str):\n",
    "            logger.error(\"Invalid URL: must be a non-empty string\")\n",
    "            raise ValueError(\"URL must be a non-empty string\")\n",
    "        self._parsed = urlparse(self.url)\n",
    "        if not (self._parsed.scheme or self._parsed.path) and not os.path.exists(self.url):\n",
    "            logger.error(\"Invalid URL: %s\", self.url)\n",
    "            raise ValueError(f\"Invalid URL: {self.url}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"this class helps in registering new mappings,lookup source by extension,\n",
    "lookup for reader by source type,checks if all the sources have readers \"\"\"\n",
    "class ReaderRegistry:\n",
    "    \"\"\"Manages mappings of schemes/extensions to source types and readers.\"\"\"\n",
    "    _default_mappings = None\n",
    "    _default_readers = None\n",
    "\n",
    "    def __init__(self):\n",
    "        logger.info(\"Creating new ReaderRegistry\")\n",
    "        if ReaderRegistry._default_mappings is None:\n",
    "            ReaderRegistry._default_mappings = {}\n",
    "            ReaderRegistry._default_readers = {}\n",
    "            self._mappings = ReaderRegistry._default_mappings\n",
    "            self._readers = ReaderRegistry._default_readers\n",
    "            self._register_defaults()\n",
    "        else:\n",
    "            self._mappings = ReaderRegistry._default_mappings.copy()\n",
    "            self._readers = ReaderRegistry._default_readers.copy()\n",
    "            self._register_defaults()\n",
    "\n",
    "    def _register_defaults(self):\n",
    "        \"\"\"Register default mappings and readers.\"\"\"\n",
    "        defaults = [\n",
    "            ((\"http\", \"https\"), \"api\", ApiReader()),\n",
    "            ((\"sqlite\", \"postgresql\", \"mysql\"), \"database\", DatabaseReader()),\n",
    "            ((\"csv\", \"json\", \"parquet\", \"xlsx\"), \"file\", FileReader()),\n",
    "            ((\"gz\",), \"compressed_file\", CompressedFileReader())\n",
    "        ]\n",
    "        for keys, source_type, reader in defaults:\n",
    "            self.register(keys, source_type, reader)\n",
    "\n",
    "    def register(self, keys: tuple[str, ...], source_type: str, reader: Reader):\n",
    "        \"\"\"\n",
    "        Register schemes/extensions, source type, and reader.\n",
    "\n",
    "        Args:\n",
    "            keys: Tuple of schemes/extensions (e.g., ('zip',)).\n",
    "            source_type: Source type identifier (e.g., 'zip_file').\n",
    "            reader: Reader instance for the source type.\n",
    "\n",
    "        Raises:\n",
    "            ValueError: If inputs are invalid or mappings conflict.\n",
    "        \"\"\"\n",
    "        logger.debug(\"Registering keys %s to source type %s with reader %s\", keys, source_type, reader.__class__.__name__)\n",
    "        if not isinstance(reader, Reader):\n",
    "            logger.error(\"Invalid reader for %s: not a Reader instance\", source_type)\n",
    "            raise ValueError(f\"Reader for {source_type} must be a Reader instance\")\n",
    "        if not source_type or not isinstance(source_type, str):\n",
    "            logger.error(\"Invalid source type: %s\", source_type)\n",
    "            raise ValueError(f\"Invalid source type: {source_type}\")\n",
    "        for key in keys:\n",
    "            if not key or not isinstance(key, str):\n",
    "                logger.error(\"Invalid key: %s\", key)\n",
    "                raise ValueError(f\"Invalid key: {key}\")\n",
    "            if key in self._mappings:\n",
    "                logger.error(\"Key %s already registered\", key)\n",
    "                raise ValueError(f\"Key {key} already registered\")\n",
    "            self._mappings[key] = source_type\n",
    "        if source_type in self._readers:\n",
    "            logger.error(\"Source type %s already registered\", source_type)\n",
    "            raise ValueError(f\"Source type {source_type} already registered\")\n",
    "        self._readers[source_type] = reader\n",
    "\n",
    "    def get_source_type(self, key: str) -> Optional[str]:\n",
    "        \"\"\"Get source type for a scheme/extension.\"\"\"\n",
    "        return self._mappings.get(key)\n",
    "\n",
    "    def get_reader(self, source_type: str) -> Optional[Reader]:\n",
    "        \"\"\"Get reader for a source type.\"\"\"\n",
    "        return self._readers.get(source_type)\n",
    "\n",
    "    def validate(self):\n",
    "        \"\"\"Validate that all source types have readers.\"\"\"\n",
    "        logger.debug(\"Validating ReaderRegistry mappings\")\n",
    "        source_types = set(self._mappings.values())\n",
    "        for source_type in source_types:\n",
    "            if source_type not in self._readers:\n",
    "                logger.error(\"No reader provided for source type: %s\", source_type)\n",
    "                raise ValueError(f\"No reader provided for source type: {source_type}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"it is used to initalise the registry, validate the registry,\n",
    "validate reader,executes and read the data,\n",
    "concantenate source and target data frame ,\n",
    "checks type of data source,handles error during data reading\"\"\"\n",
    "class DataReader:\n",
    "    _default_registry = ReaderRegistry()\n",
    "\n",
    "    def __init__(self, registry: ReaderRegistry = None):\n",
    "        \"\"\"\n",
    "        Initialize DataReader with a reader registry.\n",
    "\n",
    "        Args:\n",
    "            registry: Optional ReaderRegistry instance. If None, uses the shared default registry.\n",
    "        \"\"\"\n",
    "        logger.info(\"Initializing DataReader with %s registry\", \"shared default\" if registry is None else \"custom\")\n",
    "        self._registry = registry or DataReader._default_registry\n",
    "        self._post_init()\n",
    "\n",
    "    def _post_init(self):\n",
    "        \"\"\"Validate registry after initialization.\"\"\"\n",
    "        self._registry.validate()\n",
    "\n",
    "    def register_reader(self, keys: tuple[str, ...], source_type: str, reader: Reader):\n",
    "        \"\"\"\n",
    "        Register a new reader for the DataReader instance.\n",
    "\n",
    "        Args:\n",
    "            keys: Tuple of schemes/extensions (e.g., ('zip',)).\n",
    "            source_type: Source type identifier (e.g., 'zip_file').\n",
    "            reader: Reader instance for the source type.\n",
    "\n",
    "        Raises:\n",
    "            ValueError: If inputs are invalid or mappings conflict.\n",
    "        \"\"\"\n",
    "        logger.debug(\"Registering reader for DataReader: %s -> %s\", keys, source_type)\n",
    "        self._registry.register(keys, source_type, reader)\n",
    "\n",
    "    def _select_reader(\n",
    "        self,\n",
    "        source: DataSource,\n",
    "        custom_reader: Optional[Callable[[str], pd.DataFrame] | Reader]\n",
    "    ) -> Callable[[str], pd.DataFrame] | Reader:\n",
    "        \"\"\"\n",
    "        Select and validate the reader for the given source.\n",
    "\n",
    "        Args:\n",
    "            source: DataSource with the URL.\n",
    "            custom_reader: Optional Reader or callable to override default reading.\n",
    "\n",
    "        Returns:\n",
    "            Reader or callable to read the data.\n",
    "\n",
    "        Raises:\n",
    "            ValueError: If no reader is found for the URL.\n",
    "        \"\"\"\n",
    "        logger.debug(\"Selecting reader for source: %s\", source.url)\n",
    "        reader = custom_reader or self._registry.get_reader(self._infer_source_type(source))\n",
    "        if not reader:\n",
    "            logger.error(\"No reader found for URL: %s\", source.url)\n",
    "            raise ValueError(f\"Cannot infer reader for URL: {source.url}\")\n",
    "        logger.info(\"Using reader: %s\", reader.__class__.__name__ if isinstance(reader, Reader) else type(reader).__name__)\n",
    "        return reader\n",
    "\n",
    "    def _execute_reader(\n",
    "        self,\n",
    "        reader: Callable[[str], pd.DataFrame] | Reader,\n",
    "        url: str,\n",
    "        **kwargs\n",
    "    ) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Execute the reader to read data from the source.\n",
    "\n",
    "        Args:\n",
    "            reader: Reader or callable to read the data.\n",
    "            url: Source URL.\n",
    "            **kwargs: Additional parameters for the reader.\n",
    "\n",
    "        Returns:\n",
    "            pandas DataFrame with the read data.\n",
    "        \"\"\"\n",
    "        logger.debug(\"Executing reader for %s with kwargs: %s\", url, kwargs)\n",
    "        return reader.read(url, **kwargs) if isinstance(reader, Reader) else reader(url, **kwargs)\n",
    "\n",
    "    def _concatenate_df(\n",
    "        self,\n",
    "        df: pd.DataFrame,\n",
    "        target_df: pd.DataFrame,\n",
    "        concatenate: bool\n",
    "    ) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Concatenate the read DataFrame with the target DataFrame if required.\n",
    "\n",
    "        Args:\n",
    "            df: DataFrame read from the source.\n",
    "            target_df: DataFrame to concatenate with.\n",
    "            concatenate: Whether concatenation is required.\n",
    "\n",
    "        Returns:\n",
    "            Concatenated pandas DataFrame.\n",
    "        \"\"\"\n",
    "        return pd.concat([df, target_df], ignore_index=True) if concatenate else df\n",
    "\n",
    "    def _handle_read_error(self, e: Exception, url: str, concatenate: bool, locals_dict: dict) -> None:\n",
    "        \"\"\"\n",
    "        Handle errors during data reading, including concatenation failures.\n",
    "\n",
    "        Args:\n",
    "            e: Exception raised during reading.\n",
    "            url: Source URL.\n",
    "            concatenate: Whether concatenation was attempted.\n",
    "            locals_dict: Local variables for checking DataFrame existence.\n",
    "\n",
    "        Raises:\n",
    "            ValueError: With appropriate error message for reading or concatenation failure.\n",
    "        \"\"\"\n",
    "        logger.error(\"Failed to read from %s: %s\", url, str(e))\n",
    "        if concatenate and isinstance(e, ValueError) and 'df' in locals_dict:\n",
    "            raise ValueError(f\"Failed to concatenate with target_df: {e}\")\n",
    "        raise ValueError(f\"Failed to read from {url}: {e}\")\n",
    "\n",
    "    def read_data(\n",
    "        self,\n",
    "        source: DataSource,\n",
    "        target_df: Optional[pd.DataFrame] = None,\n",
    "        custom_reader: Optional[Callable[[str], pd.DataFrame] | Reader] = None,\n",
    "        **kwargs\n",
    "    ) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Read data from a source into a DataFrame, optionally concatenating with an existing DataFrame.\n",
    "\n",
    "        Args:\n",
    "            source: DataSource with the URL.\n",
    "            target_df: Optional DataFrame to concatenate with.\n",
    "            custom_reader: Optional Reader or callable to override default reading.\n",
    "            **kwargs: Additional parameters for the reader.\n",
    "\n",
    "        Returns:\n",
    "            pandas DataFrame with the read data.\n",
    "\n",
    "        Raises:\n",
    "            ValueError: If the reader is invalid or reading/concatenation fails.\n",
    "            requests.HTTPError: If an API request fails.\n",
    "            sqlalchemy.exc.SQLAlchemyError: If a database query fails.\n",
    "            pandas.errors.ParserError: If file parsing fails.\n",
    "        \"\"\"\n",
    "        logger.info(\"Reading data from source: %s\", source.url)\n",
    "        concatenate = target_df is not None\n",
    "        target_df = pd.DataFrame() if target_df is None else target_df\n",
    "        try:\n",
    "            reader = self._select_reader(source, custom_reader)\n",
    "            df = self._execute_reader(reader, source.url, **kwargs)\n",
    "            df = self._concatenate_df(df, target_df, concatenate)\n",
    "        except (ValueError, pd.errors.ParserError, requests.HTTPError, sqlalchemy.exc.SQLAlchemyError) as e:\n",
    "            self._handle_read_error(e, source.url, concatenate, locals())\n",
    "        logger.info(\"Successfully read data from %s\", source.url)\n",
    "        return df\n",
    "\n",
    "    def _infer_source_type(self, source: DataSource) -> str:\n",
    "        \"\"\"\n",
    "        Infer the source type from a DataSource using its parsed URL.\n",
    "\n",
    "        Args:\n",
    "            source: DataSource with the URL and parsed result.\n",
    "\n",
    "        Returns:\n",
    "            Source type ('api', 'database', 'file', 'compressed_file').\n",
    "\n",
    "        Raises:\n",
    "            ValueError: If the source type cannot be inferred.\n",
    "        \"\"\"\n",
    "        logger.debug(\"Inferring source type for URL: %s (scheme=%s, path=%s)\", source.url, source._parsed.scheme, source._parsed.path)\n",
    "        parsed = source._parsed\n",
    "        path = parsed.path if parsed.path else source.url\n",
    "        extension = path.split(\".\")[-1].lower() if \".\" in path else \"\"\n",
    "        key = parsed.scheme if extension == \"\" else extension\n",
    "        source_type = self._registry.get_source_type(key)\n",
    "        if source_type is None and extension == \"\" and os.path.exists(source.url):\n",
    "            logger.warning(\"No extension for URL %s, defaulting to file source type\", source.url)\n",
    "            source_type = \"file\"\n",
    "        if source_type is None:\n",
    "            logger.error(\"Cannot infer source type for URL: %s\", source.url)\n",
    "            raise ValueError(f\"Cannot infer source type from URL: {source.url}\")\n",
    "        logger.debug(\"Inferred source type: %s\", source_type)\n",
    "        return source_type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'data_reader'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[70], line 12\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mrequests\u001b[39;00m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mrequests\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mexceptions\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m HTTPError\n\u001b[0;32m---> 12\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mdata_reader\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (  \u001b[38;5;66;03m# Replace your_module\u001b[39;00m\n\u001b[1;32m     13\u001b[0m     DataReader,\n\u001b[1;32m     14\u001b[0m     DataSource,\n\u001b[1;32m     15\u001b[0m     Reader,\n\u001b[1;32m     16\u001b[0m     ReaderRegistry,\n\u001b[1;32m     17\u001b[0m     ApiReader,\n\u001b[1;32m     18\u001b[0m     DatabaseReader,\n\u001b[1;32m     19\u001b[0m     FileReader,\n\u001b[1;32m     20\u001b[0m     CompressedFileReader,\n\u001b[1;32m     21\u001b[0m     configure_logging,\n\u001b[1;32m     22\u001b[0m )\n\u001b[1;32m     24\u001b[0m \u001b[38;5;66;03m# Configure logging for tests (optional, but helpful for debugging)\u001b[39;00m\n\u001b[1;32m     25\u001b[0m configure_logging(level\u001b[38;5;241m=\u001b[39mlogging\u001b[38;5;241m.\u001b[39mDEBUG, logging_enabled\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, log_file\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtest_datareader.log\u001b[39m\u001b[38;5;124m'\u001b[39m, debug_enabled\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'data_reader'"
     ]
    }
   ],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
